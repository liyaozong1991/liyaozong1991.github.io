---
layout: post
comments: true
categories: 机器学习
---
反向传播算法是神经网络中使用最普遍的优化算法，这里做一个简单的总结。对于给定的训练集

$$
D=\{(x_{1}, y_{1}), (x_{2}, y_{2}), ... , (x_{m}, y_{m})\}, x_i \in \mathbb{R}^{d}, y_i \in \mathbb{R}^l
$$

假设有一个三层神经网络，有d个输入神经元，l个输出神经元，中间隐层数量为q。隐层和输出层均采用sigmod作为激活函数。设输入层到隐层之间的转移矩阵为V[d\*l]，隐层到输出层之间的转移矩阵W[l\*q]。

对于一个训练数据$(x_k, y_k)$，假定神经网络的输出为:

$$
\hat{y}_{k}=(\hat{y}_{1}^{k}, \hat{y}_{2}^{k},...,\hat{y}_{l}^{k})
$$

网络的计算过程可以通过如下几个公式表示：

$$
\begin{aligned}
\alpha_{h}&=\sum_{i=1}^{d}v_{ih}x_{i}\\
b_{h}&=sigmod(\alpha_h)\\
\beta_{j}&=\sum_{h=1}^{q}\omega_{hj}b_{h}\\
\hat{y}_{j}&=sigmod(\beta_{j})
\end{aligned}
$$

其中，$\alpha_h$为第h个隐层神经元输入，$b_h$为第h个隐层神经元输出，$\beta_j$为第j个输出神经元的输入，$\hat{y}_j$为第j个神经元的输出。

我们定义误差函数为均方误差：

$$
E_{k}=\frac{1}{2}\sum_{j-1}^{l}(\hat{y}_{j}^{k}-y_{j}^{k})^{2}
$$

对于$\Delta\omega_{ij}=-\eta\frac{\partial E_{k}}{\partial \omega_{hj}}$可以通过链式法则求解：

$$
\frac{\partial E_{k}}{\partial \omega_{hj}}=\frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}}.\frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}}.\frac{\partial \beta_{j}}{\partial \omega_{hj}}
$$

这三个部分分别进行求导如下：

$$
\begin{aligned}
\frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}}&=\hat{y}_{j}^{k}-y_{j}^{k}\\
\frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}}&=\hat{y}_{j}^{k}(1-\hat{y}_{j}^{k})\\
\frac{\partial \beta_{j}}{\partial \omega_{hj}}&=b_{h}
\end{aligned}
$$

设

$$
g_{i}=\hat{y}_{j}^{k}(1-\hat{y}_{j}^{k})(\hat{y}_{j}^{k}-y_{j}^{k})
$$

则

$$
\Delta \omega_{hj}=-\eta g_{i}b_{h}
$$

同理，对于$\Delta \upsilon_{ih}$，有：

$$
\Delta \upsilon_{ih}=-\eta e_{h}x_{i}
$$

完整的推导过程如下：

$$
\Delta \upsilon_{ih}=-\eta \frac{\partial E_{k}}{\partial \upsilon_{ih}}
$$

其中：

$$
\frac{\partial E_{k}}{\partial \upsilon_{ih}}=\frac{\partial E_{k}}{\partial b_{h}}.\frac{\partial b_{h}}{\partial \alpha_{h}}.\frac{\partial \alpha_{h}}{\partial \upsilon_{ih}}
$$

分别计算上面三项：

$$
\begin{aligned}
\frac{\partial E_{k}}{\partial b_{h}}&=\sum_{j=1}^{l}\frac{\partial E_{k}}{\partial \beta_{j}}\frac{\partial \beta_j}{\partial b_{h}}=\sum_{j=1}^{l}\omega_{hj}g_{j}\\
\frac{\partial b_{h}}{\partial \alpha_{h}}&=b_{h}(1-b_{h})\\
\frac{\partial \alpha_{h}}{\partial \upsilon_{ih}}&=x_{i}
\end{aligned}
$$

设置

$$
e_h=b_h(1-b_h)\sum_{j=1}^{l}\omega_{hj}g_j
$$

最终

$$
\Delta \upsilon_{ih}=-\eta e_{h}x_{i}
$$

以上是针对一条数据计算梯度，神经网络一般采用的是随机梯度下降算法，对一整批数据统一计算梯度后更新参数。此时，误差变为所有样本误差均值：

$$
E=\frac{1}{m}\sum_{k=1}^{m}E_k
$$
