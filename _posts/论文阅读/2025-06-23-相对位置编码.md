---
categories: [机器学习]
tags: [大模型]     # TAG names should always be lowercase
math: true
title: 相对位置编码
---

论文名字：Self-Attention with Relative Position Representations

绝对位置编码有明显的局限性，不能处理超过训练数据长度的序列，token之间的位置关系，表达不清晰。

1. 自注意力子层输出计算:

$$
z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}\right) 
$$

2. 注意力权重系数计算（softmax 函数）：

$$
\alpha_{i j}=\frac{\exp e_{i j}}{\sum_{k=1}^{n} \exp e_{i k}}
$$

3. 其中$e_{i,j}$计算如下：

$$   
e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}\right)^{T}}{\sqrt{d_{z}}}
$$

上面是原始的self-attention计算过程，下面对原始的公式进行简单修改：

$$
e_{i j}=\frac{x_{i} W^{Q}\left(x_{j} W^{K}+a_{i j}^{K}\right)^{T}}{\sqrt{d_{z}}}
$$

第一步，修改原始权重，原始公式计算两个元素的权重，只考虑了元素本身，没有考虑位置，这里添加的一个位置偏执项，$a_{i,j}^{K}$ 注意i->k表示的位置变化的值，是有方向的。

这个公式可以被拆分，拆分后如下：

$$
e_{i j}=\frac{x_{i} W^{Q}\left(x_{j} W^{K}\right)^{T}+x_{i} W^{Q}\left(a_{i j}^{K}\right)^{T}}{\sqrt{d_{z}}}
$$

然后考虑相对位置的数据计算如下：

$$
z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}+a_{i j}^{V}\right)
$$ 

在原始计算公式中，获取i位置对所有j位置的权重后，和V相乘累加即可得到对应i位置的输出，这里又额外引入了一个偏置项，$a_{i,j}^{V}$ ，它同样是有方向的。

第一截断函数：

$$
clip(x, k)=\max (-k, \min (k, x))
$$ 

其中，需要额外引入的两个矩阵定义如下：

$$
a_{i j}^{K}=w_{\text {clip }(j-i, k)}^{K}
$$

$$
a_{i j}^{V}=w_{\text {clip }(j-i, k)}^{V}
$$ 

