# Softmax损失函数中的温度系数

## 温度系数的作用
在Softmax损失函数中引入温度系数（Temperature Scaling）的目的是通过调节概率分布的“尖锐”或“平滑”程度，优化模型的训练或推理效果。具体作用如下：

1. **控制输出分布的平滑度**
    - **温度系数公式**：  
      
   $$ p_i = \frac{\exp(z_i / \tau)}{\sum_{j=1}^C \exp(z_j / \tau)}$$
2. 
      其中 $z_i$ 为模型输出的原始得分（logits），$\tau$ 为温度系数，$C$ 为类别数。
    - **温度系数的影响**：
        - $\tau > 1$：放大低概率类别的输出，使分布更平滑（缓解模型过度自信）。
        - $\tau < 1$：放大高概率类别的输出，使分布更尖锐（增强模型置信度）。

2. **知识蒸馏（Knowledge Distillation）**  
   在教师-学生模型中，教师模型使用较高的温度系数生成“软标签”（Soft Labels），包含类别间相似性信息；学生模型通过匹配软标签学习泛化能力更强的特征。

3. **模型校准**  
   调节温度系数可校准模型输出的置信度，使预测概率更贴近真实正确率（尤其在分类任务中）。

---

## 实际应用案例

### 1. 知识蒸馏
- **场景**：将复杂教师模型的知识迁移到轻量学生模型。
- **调节方法**：
    1. 教师模型使用高温（如 $\tau=5$）生成软标签。
    2. 学生模型训练时使用相同温度计算损失函数。
    3. 推理时恢复 $\tau=1$。
- **示例代码**：
  ```
  # 教师模型输出（高温）
  teacher_logits = ...  # 教师模型的原始输出
  temperature = 5
  soft_labels = torch.softmax(teacher_logits / temperature, dim=1)
  
  # 学生模型训练损失
  student_logits = ...  # 学生模型的原始输出
  loss = torch.nn.KLDivLoss()(
      torch.log_softmax(student_logits / temperature, dim=1),
      soft_labels
  )
  ```