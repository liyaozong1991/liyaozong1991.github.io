---
layout: post
comments: true
categories: 机器学习
---
注意：公式中所有的<img src="http://chart.googleapis.com/chart?cht=tx&chl=\div">便是+号，因为+在公式中显示不出来。

#### 一、损失函数
在机器学习中，损失函数用来评估预测值与真实值之间的差异程度，有的文章会区分损失函数（loss function）和代价函数（cost function），损失函数表示单个样本差异，而代价函数用来表示整体差异，本文统一叫做损失函数了。

在处理实际问题时，只优化损失函数是不够的，有时还需要加上限制项，限制模型参数之间的关系或者限制参数的复杂度用来防止过拟合。损失函数可以理解为是目标函数的一部分。

具体公式如下：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=Obj.=Loss_{\theta}(Y,X)\div \Omega(\theta)">

损失函数主要有如下几种：

1. 对数损失函数
2. 平方损失函数
3. 指数损失函数
4. Hinge损失函数

下面一次介绍这几种损失函数

##### 1、对数损失函数
对数损失函数的标准形式为：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=L(Y,P(Y|X))=-log(P(Y|X))">

其中,p(y\|x)代表样本X正确分布的概率，对数损失函数是利用极大似然的思想，求解最优参数，使得样本的分布符合训练数据的概率最大。

代表应用LR
##### 2、平方损失函数（最小二乘法）
平方损失函数的形式是：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=L(Y,f(X))=(Y-f(X))^2">

最小二乘法直接用平均L2距离衡量损失，是一种比较直观的方法。代表应用GBDT

##### 3、指数损失函数
指数损失函数的形式是：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=L(Y,f(X))=e^{-Yf(X)}">

应用Adaboost

##### 4、Hinge损失函数
Hinge损失函数形式为：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=L(Y)=max(0, 1-tY)">

其中，t为目标值(+1, -1)，Y是分类器输出的预测值，其含义为，当t与Y符号相同时(预测正确)，并且abs(Y)>=1时，hinge loss为0；当t与Y符号相反时，hinge loss随着Y的值线性增大。

应用SVM，这个相对特殊一点

#### 二、激活函数

##### 1、激活函数的作用
激活函数主要用于神经网络中，主要作用是引入非线性变换，增强网络的表达能力。如果没有激活函数，那么无论多少层的网络，都可以等价位一层的线性网络，表达能力很弱，无法解决基本的异或问题。

##### 2、 激活函数的特点
1. 首先，激活函数是非线性函数，因为其主要作用是引入非线性变换。
2. 根据万能逼近定理提出的要求，理想的激活函数具有如下几个特点：非常数、有界、单调递增、连续。
3. 因为神经网络一般使用反向传播算法+梯度法求解，所以要求激活函数必须是可导的，数学上精确的要求是**几乎处处可到**(不可导的点只有有限个，或者无限可列个)。具体还要考虑梯度消失，梯度爆炸问题。

##### 3、常见的激活函数
1. sigmod函数
<img src="http://chart.googleapis.com/chart?cht=tx&chl=f(x)=\frac{1}{1-e^{-x}} ">

sigmod函数相对平滑，导函数形式简单，但是它是非线性的，且输出值在0-1之间，不以0为中心对称。在深层网络中，容易出现梯度消失。函数本身不复杂，但是有指数运算，开销相对较大。
2. TanH函数
<img src="http://chart.googleapis.com/chart?cht=tx&chl=tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}\div e^{-z}}">

Tanh函数与sigmod函数类似，存在梯度消失的现象，但是其输出值被映射在(-1,1)之间。
3. Relu函数
<img src="http://chart.googleapis.com/chart?cht=tx&chl=relu(z)=max(0,z)">

Relu形式简单，计算开销小，不存在梯度消失问题，但是也存在Relu坏死和不稳定的问题。

4. softmax函数

<img src="http://chart.googleapis.com/chart?cht=tx&chl=S_{i}=\frac{e^{x_i}}{\sum e^{x_i}}">

softmax函数的输出值也在0-1之间，并且对所有的输出值进行归一化，适合多分类的问题。一般用在多分类神经网络的最后一层。
