# 深度学习常用损失函数与激活函数对比总结

---

## 一、常用损失函数及使用场景

### 1. 均方误差（MSE, Mean Squared Error）
- **公式**：  

  $$ L = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 $$

- **使用场景**：回归任务（如房价预测、温度预测）。
- **优点**：计算简单，梯度平滑，收敛快。
- **缺点**：对异常值敏感，可能导致梯度爆炸。
- **图像特点**：二次函数曲线，误差越大损失增长越快。

### 2. 平均绝对误差（MAE, Mean Absolute Error）
- **公式**：

  $$ L = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i| $$

- **使用场景**：回归任务（对异常值鲁棒的场景，如金融数据预测）。
- **优点**：对异常值鲁棒，梯度稳定。
- **缺点**：收敛速度较慢。
- **图像特点**：线性增长，误差与损失呈正比。

### 3. Huber损失（Smooth L1 Loss）
- **公式**：  

  $$ L = \begin{cases}
  0.5 (y_i - \hat{y}_i)^2 & \text{if } |y_i - \hat{y}_i| \le \delta \\
  \delta (|y_i - \hat{y}_i| - 0.5 \delta) & \text{otherwise}
  \end{cases} $$

- **使用场景**：回归任务（平衡MSE和MAE的优势，适合含噪声数据）。
- **优点**：结合MSE的平滑梯度和MAE的鲁棒性。
- **缺点**：需手动调整超参数$\delta$。

### 4. 交叉熵损失（Cross-Entropy Loss）
- **二分类公式**：  
  
  $$ L = -\frac{1}{n} \sum_{i=1}^n [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] $$

- **多分类公式**：  
  
  $$ L = -\frac{1}{n} \sum_{i=1}^n \sum_{c=1}^C y_{i,c} \log(\hat{y}_{i,c}) $$

- **使用场景**：分类任务（二分类用Sigmoid+二元交叉熵，多分类用Softmax+交叉熵）。
- **优点**：梯度更新高效，适合概率分布优化。
- **缺点**：对类别不平衡敏感。
- **图像特点**：对数函数曲线，预测偏离真实值时损失急剧上升。

### 5. Hinge损失（支持向量机损失）
- **公式**：  

$$ L = \max(0, 1 - y_i \cdot \hat{y}_i) $$

- **使用场景**：二分类任务（如SVM模型）。
- **优点**：生成稀疏解，适合支持向量机。
- **缺点**：仅支持二分类，对噪声敏感。
- **图像特点**：分段线性函数，当预测正确且置信度大于1时损失为0。

---

## 二、常用激活函数及使用场景

### 1. Sigmoid
- **公式**：  
  
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$

- **使用场景**：二分类输出层、概率映射（如逻辑回归）:cite[1]:cite[4]。
- **优点**：输出范围[0,1]，适合概率解释。
- **缺点**：梯度消失问题，输出非零均值。
- **图像特点**：S型曲线，两端饱和区梯度趋近于0。

### 2. Tanh
- **公式**：  
  
$$ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

- **使用场景**：隐藏层（比Sigmoid更适合梯度传播）。
- **优点**：输出范围[-1,1]，零均值缓解梯度问题。
- **缺点**：梯度消失问题仍存在。
- **图像特点**：S型曲线，中心对称。

### 3. ReLU（Rectified Linear Unit）
- **公式**：  
  
$$ \text{ReLU}(x) = \max(0, x) $$

- **使用场景**：隐藏层（CNN、全连接网络的默认选择）。
- **优点**：计算高效，缓解梯度消失。
- **缺点**：“死亡神经元”问题（输入为负时梯度为0）。
- **图像特点**：左半轴恒为0，右半轴线性增长。

### 4. Leaky ReLU
- **公式**：  
  
$$ \text{LeakyReLU}(x) = \begin{cases}
  x & \text{if } x > 0 \\
  \alpha x & \text{otherwise}
  \end{cases} $$

- **使用场景**：改进ReLU的死亡神经元问题（如GANs）。
- **优点**：缓解死亡神经元问题，$\alpha$通常设为0.01。
- **缺点**：需调参选择$\alpha$。

### 5. Softmax
- **公式**：  
  
$$ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^C e^{x_j}} $$

- **使用场景**：多分类输出层（如图像分类）。
- **优点**：输出概率分布，总和为1。
- **缺点**：数值不稳定（需配合Log-Sum-Exp技巧）。

---

## 三、对比表格

| **类型**       | **函数名称**       | **主要场景**               | **优点**                          | **缺点**                          |
|----------------|--------------------|---------------------------|-----------------------------------|-----------------------------------|
| **损失函数**   | MSE                | 回归任务                  | 梯度平滑，收敛快                 | 对异常值敏感                      |
|                | MAE                | 鲁棒回归                  | 对异常值鲁棒                     | 收敛慢                            |
|                | Huber              | 含噪声的回归              | 平衡MSE和MAE                     | 需调参                            |
|                | 交叉熵             | 分类任务                  | 高效优化概率分布                 | 类别不平衡敏感                    |
|                | Hinge              | SVM二分类                 | 稀疏解，支持最大间隔分类         | 仅支持二分类                      |
| **激活函数**   | Sigmoid            | 二分类输出层              | 概率解释清晰                    | 梯度消失                          |
|                | Tanh               | 隐藏层                    | 零均值，梯度传播更优            | 梯度消失                          |
|                | ReLU               | 隐藏层（默认选择）        | 计算高效，缓解梯度消失           | 死亡神经元                        |
|                | Leaky ReLU         | 改进ReLU的隐藏层          | 缓解死亡神经元问题               | 需调参                            |
|                | Softmax            | 多分类输出层              | 输出概率分布                     | 数值不稳定                        |

---

## 四、函数图像特征总结
1. **Sigmoid/Tanh**：S型曲线，两端梯度趋近于0，易导致梯度消失。
2. **ReLU系列**：分段线性函数，正区间梯度恒定，负区间梯度为0（ReLU）或小斜率（Leaky ReLU）。
3. **MSE vs MAE**：MSE为抛物线，MAE为V型直线；Huber损失在误差小时为抛物线，大时为直线。
4. **交叉熵**：对数曲线，预测错误时损失急剧上升。

### 损失函数图像
![img.png](/static/img/img.png)

### 激活函数图像 
![img.png](/static/img/img1.png)
---