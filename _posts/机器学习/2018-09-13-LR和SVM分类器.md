---
layout: post
comments: true
categories: 机器学习
---
在机器学习领域，LR和SVM应该算是hello级别的分类器，简单总结下。

注意：公式中所有的<img src="http://chart.googleapis.com/chart?cht=tx&chl=\div">便是+号，因为+在公式中显示不出来。
#### 一、LR
LR分类器用于解决01分类问题，是一个线性的2分类器，能力较弱，但是速度较快。logistic函数用来表示样本为正样本(y=1)的概率，具体表达式如下：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=h_{\theta }(x)=\frac{1}{1\div e^{-X^{T}\theta}}">

logistic函数的导入如下：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=h_{\theta }^{'}(x)=h_{\theta }(x)(1-h_{\theta}(x))">

样本被正确分类的概率可以表示为：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=P(y=1|x) = h_{\theta}(x)\\
P(y=0|x) = 1 - h_{\theta}(x)">

可以将这两个公式写在一起：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=P(y|x) = h_{\theta}(x)^{y}(1-h_{\theta}(x))^{1-y}">

所有样本均被正确分类的概率的极大似然估计的对数形式，就是我们的目标函数：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=Object=\sum_{n}^{i=1}y^{i}log(h_{\theta}(x_{i})) \div (1-y^{i})log(1-h_{\theta}(x_{i})))">

可以使用梯度法求解最大值，公式对参数的每一个维度分别求偏导数，最终更新公式为：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=\theta_{j} := \theta_{j} + \lambda\frac{1}{m} \sum_{i=1}^{n}(y^{(i)} - h_{\theta}(x^{(i)}))x_{j}^{(i)}">

#### 二、SVM
理解LR分类器，直接从概率的角度理解就可以了，理解SVM分类器，则要从几何角度理解，SVM就是需求最佳分类面，使得正负样本之间的间隔最大。SVM的原理很简单，但是求解过程却很复杂。

分类面函数为<img src="http://chart.googleapis.com/chart?cht=tx&chl=f(x)=\omega ^{T}X\div b">，如果函数值大于0，则判定为正样本(y=1)，否则判定为负样本(y=-1)，这样有<img src="http://chart.googleapis.com/chart?cht=tx&chl=y*f(x)>=0=|f(x)|">，因为|f(x)|可以表示的是函数距离，同时缩放系数，函数本质并不发生变化，所以需要加入限制条件<img src="http://chart.googleapis.com/chart?cht=tx&chl=||\omega ||^{2}=K_{1}">后进行优化，最终目标函数变为：

<img src="http://chart.googleapis.com/chart?cht=tx&chl=obj. = max(y*f(x)),s.t.,||\omega ||^{2}=K_{1}">

这个目标函数不太好求解，可以进行等价变换：
<img src="http://chart.googleapis.com/chart?cht=tx&chl=obj. = min(||\omega ||^{2}),s.t.,y*f(x)>=K_{2}=1">

这个目标函数可以通过引入拉格朗日对偶变换求解，最终转换为求解凸优化问题，具体推到太麻烦了，不看了。
