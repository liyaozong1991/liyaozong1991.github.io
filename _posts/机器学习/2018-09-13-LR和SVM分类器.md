---
layout: post
comments: true
categories: 机器学习
---
在机器学习领域，LR和SVM应该算是hello world级别的分类器，简单总结下。

#### 一、LR
LR分类器用于解决01分类问题，是一个线性的2分类器，能力较弱，但是速度较快。logistic函数用来表示样本为正样本(y=1)的概率，具体表达式如下：

$$
h_{\theta }(x)=\frac{1}{1+e^{-X^{T}\theta}}
$$

logistic函数的导入如下：

$$
h_{\theta }^{'}(x)=h_{\theta }(x)(1-h_{\theta}(x))
$$

样本被正确分类的概率可以表示为：

$$
\begin{aligned}
P(y=1|x) &= h_{\theta}(x)\\
P(y=0|x) &= 1 - h_{\theta}(x)
\end{aligned}
$$

可以将这两个公式写在一起：

$$
P(y|x) = h_{\theta}(x)^{y}(1-h_{\theta}(x))^{1-y}
$$

所有样本均被正确分类的概率的极大似然估计的对数形式，就是我们的目标函数：

$$
Object=\sum_{n}^{i=1}y^{i}log(h_{\theta}(x_{i}))+ (1-y^{i})log(1-h_{\theta}(x_{i})))
$$

可以使用梯度法求解最大值，公式对参数的每一个维度分别求偏导数，最终更新公式为：

$$
\theta_{j} := \theta_{j} + \lambda\frac{1}{m} \sum_{i=1}^{n}(y^{(i)} - h_{\theta}(x^{(i)}))x_{j}^{(i)}
$$

#### 二、SVM
理解LR分类器，直接从概率的角度理解就可以了，理解SVM分类器，则要从几何角度理解，SVM就是需求最佳分类面，使得正负样本之间的间隔最大。SVM的原理很简单，但是求解过程却很复杂。
分类面函数为

$$
f(x)=\omega ^{T}X+b
$$

如果函数值大于0，则判定为正样本(y=1)，否则判定为负样本(y=-1)，这样有

$$
y*f(x)\ge 0=|f(x)|
$$

因为f(x)可以表示的是函数距离，同时缩放系数，函数本质并不发生变化，所以需要加入限制条件。

$$
||\omega ||^{2}=K_{1}
$$

最终目标函数变为：

$$
chl=obj. = max(y*f(x)),s.t.,||\omega ||^{2}=K_{1}
$$

这个目标函数不太好求解，可以进行等价变换：

$$
obj. = min(||\omega ||^{2}),s.t.,y*f(x)\ge K_{2}=1
$$

这个目标函数可以通过引入拉格朗日对偶变换求解，最终转换为求解凸优化问题。
