---
layout: post
comments: true
categories: 机器学习
---

# PPO vs DPO 详细对比

| **对比维度**       | **PPO (Proximal Policy Optimization)**                                                                 | **DPO (Direct Preference Optimization)**                                                                 |
|--------------------|-------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
| **核心目标**       | 通过限制策略更新幅度，最大化累积奖励。                                                               | 直接通过偏好数据优化策略，无需显式奖励模型。                                                             |
| **优化目标**       | 剪切替代目标（Clipped Surrogate Objective）：限制新旧策略的概率比差异，避免过大更新。                | 偏好数据对数似然最大化：直接优化策略，使其生成偏好结果的概率高于非偏好结果。                             |
| **依赖组件**       | 需奖励模型（或环境反馈）计算奖励信号。                                                               | 无需奖励模型，直接依赖成对偏好数据（如人类标注的A/B结果对比）。                                          |
| **训练流程**       | 1. 与环境交互收集数据。<br>2. 多轮次更新策略，计算奖励和梯度。<br>3. 需平衡探索与利用。              | 1. 直接使用静态偏好数据集。<br>2. 单阶段监督式优化，类似分类任务。<br>3. 无需在线交互。                  |
| **数据需求**       | 需大量交互数据（在线生成），依赖奖励模型的准确性。                                                   | 需高质量静态偏好数据（离线标注），数据质量直接影响结果。                                                 |
| **计算效率**       | 较高计算成本：需多轮策略更新和奖励模型推理。                                                         | 较低计算成本：单阶段训练，无需复杂奖励模型或在线交互。                                                   |
| **调参复杂度**     | 需调整剪切系数、学习率、KL散度惩罚等。                                                               | 超参数较少（如温度系数），更易收敛。                                                                     |
| **典型应用场景**   | 游戏AI（如Dota2）、机器人控制等需在线交互的任务。                                                   | 文本生成（如ChatGPT微调）、推荐系统等依赖人类偏好的任务。                                                |

---

## **实际使用区别示例：文本生成任务**

### **场景描述**
微调大语言模型（LLM）生成更符合人类偏好的回答。

---

### **使用PPO的流程**
1. **数据准备**：
    - 收集人类标注的偏好数据（如回答A vs 回答B，标注哪个更好）。
    - 训练奖励模型（Reward Model, RM），将偏好数据转化为标量奖励。

2. **训练过程**：
    - **步骤1**：当前策略生成回答，通过RM计算奖励。
    - **步骤2**：计算替代目标（Clipped Objective），限制策略更新幅度。
    - **步骤3**：多轮迭代更新策略，需平衡KL散度惩罚防止策略偏离初始模型太远。

   ```
   # 伪代码示例：PPO更新核心逻辑
   for epoch in range(num_epochs):
       trajectories = generate_data(policy)
       rewards = reward_model(trajectories)
       advantages = compute_advantages(rewards)
       policy.update(trajectories, advantages, clip_ratio=0.2)
   ```
   

3. **挑战**：
    - 奖励模型的准确性：需要高质量的偏好数据，否则可能导致策略偏离人类偏好。
    - 计算成本：PPO需要多轮策略更新和奖励模型推理，计算成本较高。
    - 调参复杂度：需调整剪切系数、学习率、KL散度惩罚等。

### 使用DPO的流程

1. **数据准备**：
    - 收集人类标注的偏好数据（如回答A vs 回答B，标注哪个更好）。
2. **训练过程**：
    - **步骤1**：将偏好数据转化为损失函数，直接优化策略参数。
    - **步骤2**：最大化偏好回答的概率，最小化非偏好回答的概率（通过Bradley-Terry模型等）。
   
    ```
    # 伪代码示例：DPO更新核心逻辑
    def dpo_loss(policy, preferred_data, rejected_data):
        preferred_logprobs = policy(preferred_data).logprobs
        rejected_logprobs = policy(rejected_data).logprobs
        loss = -log_sigmoid(preferred_logprobs - rejected_logprobs)
        return loss.mean()
   ```
   
3. **挑战**：
    - 数据质量：需高质量的偏好数据，否则可能导致策略偏离人类偏好。

