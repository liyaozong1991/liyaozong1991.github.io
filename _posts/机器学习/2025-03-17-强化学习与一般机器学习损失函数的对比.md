---
layout: post
comments: true
categories: 机器学习
---

# 强化学习与一般机器学习损失函数的对比

## 1. 一般机器学习（监督学习）的损失函数

### 核心目标
最小化模型预测与真实标签之间的差异。

### 常见形式
- **分类任务**：交叉熵损失（Cross-Entropy Loss）

  $$\mathcal{L} = -\sum y_i \log p_i$$

  其中 $y_i$ 是真实标签，$p_i$ 是预测概率，所有项均为非负值。  
  由于概率 $p_i \in [0,1]$，故 $-\log p_i \geq 0$。

- **回归任务**：均方误差（MSE）  
  
  $$\mathcal{L} = \frac{1}{N} \sum (y_i - \hat{y}_i)^2$$
    
  误差平方 $(y_i - \hat{y}_i)^2 \geq 0$。

### 关键性质
- **损失值始终为非负**（正值），因为误差度量天然非负。

---

## 2. 强化学习的损失函数

### 核心目标
最大化长期累积奖励（或最小化负奖励）。

### 常见形式
- **策略梯度方法**（如REINFORCE）：  
  
  $$\mathcal{L} = -\mathbb{E} \left[ \log \pi(a|s) \cdot G_t \right]$$
    
  其中回报 \(G_t\) 可能为正或负。

- **Actor-Critic方法**：  
  
  $$\mathcal{L} = -\mathbb{E} \left[ \log \pi(a|s) \cdot A(s,a) \right]$$
  
  优势函数 $A(s,a)$ 可正可负（动作优于平均时为正，反之为负）。

- **Q-Learning的TD误差**：  
  
  $$\mathcal{L} = \mathbb{E} \left[ (r + \gamma \max_{a'} Q(s',a') - Q(s,a))^2 \right]$$
  
  TD误差 $(r + \gamma Q(s',a') - Q(s,a))$ 可正可负，但损失函数本身非负。

### 关键性质
- **损失项可能为正或负**，原因包括：
    1. **环境奖励的正负性**：奖励 \(r\) 可设计为正值（鼓励）或负值（惩罚）。
    2. **优势函数与基线**：动作的“相对好坏”通过优势函数量化。
    3. **梯度方向的需要**：损失符号决定参数更新方向（如增加或减少动作概率）。

---

## 为什么监督学习损失函数为正值，而强化学习有正有负？

### 根本原因：优化目标与反馈机制不同

1. **监督学习**：
    - 目标是**最小化误差**（如预测与标签的距离）。
    - 误差度量（交叉熵、MSE）天然非负，仅反映误差大小，无方向性。

2. **强化学习**：
    - 目标是**最大化累积奖励**，需通过奖励信号提供方向性反馈。
    - 奖励可正可负：正奖励鼓励行为，负奖励抑制行为。
    - 损失函数需直接关联奖励符号（如策略梯度中，正奖励降低损失，负奖励增加损失）。

---

## 示例说明

### 监督学习（图像分类）

预测为猫的概率 $p=0.8$，真实标签为猫$y=1$，交叉熵损失为：  
  
 $$-\log(0.8) \approx 0.223 \quad (\text{正值})$$

### 强化学习（游戏控制）

策略梯度

动作导致高奖励$G_t=+10$，损失项为$-\log \pi(a|s) \cdot 10$（负值，梯度下降增加动作概率）。
动作导致惩罚 $G_t = -5$，损失项为 $-\log \pi(a|s) \cdot (-5)$（正值，梯度下降减少动作概率）。

---

## 总结

| **特性**               | **监督学习**                  | **强化学习**                  |
|-----------------------|-----------------------------|-----------------------------|
| **损失函数目标**       | 最小化预测误差               | 最大化累积奖励               |
| **损失值符号**         | 始终非负（正值）             | 可正可负                     |
| **核心设计差异**       | 误差度量无方向性             | 奖励信号驱动方向性更新       |

- **监督学习**的损失函数是纯误差度量，非负性天然合理。
- **强化学习**的损失函数需通过正负值区分策略改进方向，本质上是将奖励信号与参数更新方向绑定。