---
layout: post
comments: true
categories: 机器学习
---

FM模型在LR模型的基础上，添加了交叉特征部分的计算，使模型的拟合能力增强。其特别适合有大量稀疏ID类特征的分类问题。

特点：
1、适合特征量较大，且稀疏的情况。

2、适合one hot特征，不适合连续特征。  

3、适合做分类预估问题，不适合回归拟合问题。

FM模型：
$$
Y(x)= \omega_0 + \sum_{i = 0}^n\omega_i+\sum_{i = 0}^{n - 1}\sum_{j = i + 1}^{n}\langle v_i, v_j\rangle x_i x_j\
$$
考虑两两交叉的情况，参数规模应该是O(n^2)，FM把每个特征对应到一个k维向量，特征交叉看做的对应的向量相乘，使得参数的规模下降的O(k\*n)。

在训练时，每两个特征需要交叉训练，更新向量，所以复杂度为O(k\*n^2)。可以通过等价变换，把复杂度降低到O(k\*n)

$$
(ab + ac + bc) = \frac{1}{2}(a+b+c)^2-\frac{1}{2}(a^2+b^2+c^2)
$$

在实际更新时，由于特征的稀疏性，计算效率会非常高。

在实际使用中，数据的特征一般可以分成两部分，from侧特征和to侧特征。
在商品推荐场景，from侧特征是指用户的特征（性别，年龄，消费历史等），to侧特征是商品的特征（品类，价格，历史成交情况等）。
当完成模型训练后，to侧商品特征向量可以写入faiss，根据用户特征向量进行检索。

$$
y(x) = \sum_{i=0}^{n}v_i * \sum_{j=0}^{m}v_j
$$

上面的公式为实际检索时用的公示，与训练时差异非常大。首先没有了一阶项和偏置项，其次还没有了from侧二阶交叉项内部两两相乘和to侧二阶项内部两两相乘的结果。

这种差异带来的影响是否可以忽略，取决于具体的应用场景。在召回场景中，实际的预测值并不重要，相对的序关系才是关键。这样偏置项，from侧的一阶项和二阶内部交叉项可以忽略。to侧的一阶项和二阶内部交叉项则不能直接忽略。

有一个简单的方法可以在faiss检索时解决这个问题，即from侧向量增加一维固定值1，to侧写入faiss的向量，也固定增加一维，值为特征对应的一阶项和二阶交叉项之和。这样对于每个用户而言，检索得到商品的序关系就能保住和训练的模型完全一致。

在社交场景中，to侧一阶项和二阶内部交叉项往往会带来集中度的上涨，所以也可以考虑修改原始的训练公式，使得推荐结果更关注的匹配本身，而不是更关注to侧本身的质量。
